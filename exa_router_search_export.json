{
  "name": "Search Router Tool",
  "toolkit_id": "exa_router_search",
  "version": "1.3.0",
  "content": "\"\"\"\nTitle: Search Router Tool\nDescription: An advanced research tool with a robust retry and graceful failure mechanism.\nauthor: ShaoRou459\nauthor_url: https://github.com/ShaoRou459\nVersion: 1.3.0\nRequirements: exa_py, open_webui\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport re\nimport sys\nimport json\nimport asyncio\nimport time\nfrom typing import Any, Awaitable, Callable, Dict, List, Optional\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom urllib.parse import urlparse\n\nfrom pydantic import BaseModel, Field\n\nfrom open_webui.utils.chat import generate_chat_completion\nfrom open_webui.models.users import Users\nfrom open_webui.utils.misc import get_last_user_message\n\ntry:\n    from exa_py import Exa\n\n    EXA_AVAILABLE = True\nexcept ImportError:\n    Exa = None\n    EXA_AVAILABLE = False\n\n\n# \u2500\u2500\u2500 System Prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nSEARCH_STRATEGY_ROUTER_PROMPT_TEMPLATE = \"\"\"\nYou are a search strategy router. Analyze the user's query and conversation context to determine the best search approach.\n\nStrategies:\n- CRAWL \u2192 only if the user provided a specific URL to read.\n- STANDARD \u2192 default; quick lookup answerable with a brief web search (~5 min).\n- COMPLETE \u2192 deep, multi-source research or explicit request for in-depth analysis.\n\nRespond with your decision on a line starting with \"ANSWER: \" followed by CRAWL, STANDARD, or COMPLETE.\n\"\"\"\n\nIMPROVED_SQR_PROMPT_TEMPLATE = f\"\"\"\nYou are a search query optimizer. Convert the user's question into a natural, effective search query that would work well on Google or similar search engines.\n\nGuidelines:\n- Write like a human would search Google - natural phrases, not keyword soup\n- For news/current events: use \"latest\", \"today\", \"{datetime.now().year}\", \"recent\" naturally\n- For technical topics: include key terms but keep it readable\n- For comparisons: use \"vs\" or \"compared to\" \n- For how-to: start with \"how to\" or include \"guide\", \"tutorial\"\n- Keep it under 10 words when possible\n- Avoid excessive OR operators and site: filters unless truly needed\n\nExamples:\n- User: \"latest news about AI\" \u2192 \"latest AI news {datetime.now().year}\"\n- User: \"how do I fix my car engine\" \u2192 \"how to fix car engine problems\"\n- User: \"compare React vs Vue\" \u2192 \"React vs Vue comparison {datetime.now().year}\"\n- User: \"what is quantum computing\" \u2192 \"quantum computing explained\"\n\nOutput your optimized search query on a line starting with \"ANSWER: \"\n\"\"\"\n\nQUICK_SUMMARIZER_PROMPT = (\n    \"Using ONLY the provided context, produce a clear, organized summary that answers the user's request. \"\n    \"Do NOT include explicit citations, reference markers, or raw URLs unless the user explicitly asked for citations.\"\n)\n\n# \u2500\u2500 New Iterative Research System Prompts \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nINTRODUCTORY_QUERY_PROMPT = \"\"\"\nYou are an information-seeking specialist. Your job is to generate an introductory search query that helps understand the context and background of what the user is asking about.\n\nCURRENT DATE: {current_date}\n\nThis query should be INFORMATIONAL, not trying to answer their question directly. Think of it as \"What do I need to know about this topic first?\"\n\nExamples:\n- User: \"How do I optimize my React app performance?\" \u2192 \"React application performance optimization techniques\"\n- User: \"What's the latest news about OpenAI?\" \u2192 \"OpenAI company recent developments {current_year}\"\n- User: \"Compare investment strategies for 2024\" \u2192 \"investment strategies types overview {current_year}\"\n\nOutput your introductory query on a line starting with \"QUERY: \"\n\"\"\"\n\nOBJECTIVE_SETTING_PROMPT = \"\"\"\nYou are a research strategist. Based on the user's request and the introductory information gathered, set clear research objectives.\n\nCURRENT DATE: {current_date}\n\nAnalyze:\n1. What exactly is the user asking for?\n2. What are the key components of their request?\n3. What direction should the research take?\n\nOutput a structured analysis with:\nOBJECTIVES: [List 3-5 specific research objectives]\nRESEARCH_DIRECTION: [Brief description of the overall research approach]\nKEY_COMPONENTS: [List the main parts of the user's request that need to be addressed]\n\"\"\"\n\nITERATION_REASONING_PROMPT = \"\"\"\nYou are a research iteration planner. Based on your current knowledge and what you've found so far, reason about what to search for next.\n\nCURRENT DATE: {current_date}\n\nCurrent situation:\n- Research objectives: {objectives}\n- Previous findings summary: {previous_findings}\n- Iteration: {current_iteration} of {max_iterations}\n\nYour task:\n1. Analyze what you've learned so far\n2. Identify what's still missing\n3. Reason about the best search approach for this iteration\n4. Generate {query_count} diverse, specific search queries\n\nNote: For time-sensitive topics, include {current_year} in your queries when relevant.\n\nOutput format:\nANALYSIS: [What you've learned and what's missing]\nREASONING: [Why these specific searches will help]\nQUERIES: [\"query1\", \"query2\", \"query3\", ...]\n\"\"\"\n\nITERATION_CONCLUSION_PROMPT = \"\"\"\nYou are a research analyst. Summarize what you found in this iteration and determine next steps.\n\nCURRENT DATE: {current_date}\nITERATION: {current_iteration} of {max_iterations}\n\nProvide:\nFINDINGS_SUMMARY: [Key information discovered this iteration - be concise but comprehensive]\nPROGRESS_ASSESSMENT: [How much closer are you to answering the user's question?]\nNEXT_STEPS: [What should the next iteration focus on, or should research conclude?]\nDECISION: [CONTINUE or FINISH]\n\nNote: If this is iteration {max_iterations}, you must decide FINISH unless critical information is still missing.\n\"\"\"\n\nFINAL_SYNTHESIS_PROMPT = \"\"\"\nYou are an information organizer. Your job is to structure the research findings so the chat model can effectively answer the user's question.\n\nCURRENT DATE: {current_date}\n\nUsing the research chain and findings summaries, organize the information into a clear, comprehensive knowledge base that covers:\n- Key facts and findings relevant to the user's question\n- Important context and background information  \n- Relevant developments, especially recent ones when applicable\n- Different perspectives or approaches discovered\n- Any actionable insights or recommendations found\n\nStructure this as organized, factual information that provides the chat model with everything needed to give a complete response to the user's original question. Focus on being comprehensive and well-organized rather than directly answering.\n\nDo include raw URLs or direct quotes from sources when needed.\n\"\"\"\n\nSYNTHESIS_DECIDER_PROMPT = \"\"\"\nYou are an output formatting assistant. Decide whether to:\n- RETURN_RAW \u2192 when the user wants to read/learn/review full context (docs, articles, longer materials).\n- SYNTHESIZE \u2192 when the user asks for an answer/summary/comparison/explanation.\n\"\"\"\n\nCOMPLETE_SUMMARIZER_PROMPT = \"\"\"\nYou are an expert synthesizer. Provide a comprehensive, well-structured answer to the user's original question using the provided research context and agent notes.\n- Choose the best format (brief summary, step-by-step, comparison, etc.) based on the question.\n- Use notes to guide emphasis; pull specifics from context.\nImportant: Do NOT include explicit citations, reference markers, or raw URLs unless the user explicitly asked for citations.\n\"\"\"\n\n# \u2500\u2500 Custom Standard Search System Prompts (Unified) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nCUSTOM_SEARCH_AGENT_PROMPT = \"\"\"\nYou are a web research agent. Balance SPEED and COMPLETENESS based on query complexity.\n\nCURRENT DATE: {current_date}\nUSER REQUEST: {user_request}\nITERATION: {current_iteration} of {max_iterations}\nSOURCES CONSULTED: {source_count}\n\n{previous_findings}\n\n{new_content_section}\n\n## CRITICAL - Adapt to Query Type:\n\n**STOP EARLY (1-2 iterations) for:**\n- Broad/simple: \"latest news\", \"what's happening\", \"current events\", \"trending\"\n- General overviews where variety matters more than depth\n- When you have 5+ diverse, relevant, recent pieces of info\n- When more searching would just find similar content\n\n**CONTINUE (3+ iterations) ONLY for:**\n- Specific comparisons: \"X vs Y\", \"differences between\"\n- Technical deep-dives: \"how does X work\", \"explain the mechanism\"\n- Explicit research requests: \"thorough analysis\", \"comprehensive\", \"in-depth\"\n- When initial results are poor quality or miss key aspects user specifically asked about\n\n**Default behavior:** Prefer stopping early. Most queries need 1-2 iterations max.\n\"Latest news\" + 5 good headlines = STOP immediately.\n\n## Your Task:\n1. **Extract** key facts from new content\n2. **Evaluate** - Is this enough for THIS query type?\n3. **Decide** STOP or CONTINUE (lean toward STOP for simple queries)\n4. **If STOP** - Provide final organized summary for the chat model\n\n## Output Format:\n\nEXTRACTED_INFO: <Key facts from new content, or \"No new content\">\n\nEVALUATION: <Brief: what we have, is it enough for this query type?>\n\nDECISION: STOP or CONTINUE\n\n### If DECISION is CONTINUE:\nNEXT_QUERIES: [\"query1\", \"query2\"]\n(Exactly {query_count} queries)\n\nSTATUS_SUMMARY: <5-10 words for UI>\n\n### If DECISION is STOP:\nRESEARCH_SUMMARY:\n<Well-organized summary of ALL findings (from previous + current iteration). Format for the chat model to use - do NOT directly answer the user, just provide the research.>\n\nKEY_POINTS:\n- <Most important finding 1>\n- <Most important finding 2>\n- <etc>\n\nSTATUS_SUMMARY: Research complete\n\"\"\"\n\n\n# \u2500\u2500\u2500 Enhanced Debug System \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nfrom dataclasses import dataclass, field\nfrom contextlib import contextmanager\n\n@dataclass\nclass DebugMetrics:\n    \"\"\"Collects and tracks metrics throughout the debug session.\"\"\"\n    \n    # Timing metrics\n    start_time: float = field(default_factory=time.perf_counter)\n    operation_times: Dict[str, float] = field(default_factory=dict)\n    total_operations: int = 0\n    \n    # API/LLM metrics\n    llm_calls: int = 0\n    llm_total_time: float = 0.0\n    llm_failures: int = 0\n    \n    # Search metrics\n    search_queries: int = 0\n    urls_found: int = 0\n    urls_crawled: int = 0\n    urls_successful: int = 0\n    urls_failed: int = 0\n    \n    # Content metrics\n    total_content_chars: int = 0\n    context_truncations: int = 0\n    \n    # Error tracking\n    errors: List[str] = field(default_factory=list)\n    warnings: List[str] = field(default_factory=list)\n    \n    def add_operation_time(self, operation: str, duration: float) -> None:\n        \"\"\"Add timing data for an operation.\"\"\"\n        self.operation_times[operation] = self.operation_times.get(operation, 0) + duration\n        self.total_operations += 1\n    \n    def add_error(self, error: str) -> None:\n        \"\"\"Add an error to tracking.\"\"\"\n        self.errors.append(f\"[{datetime.now().strftime('%H:%M:%S')}] {error}\")\n    \n    def add_warning(self, warning: str) -> None:\n        \"\"\"Add a warning to tracking.\"\"\"\n        self.warnings.append(f\"[{datetime.now().strftime('%H:%M:%S')}] {warning}\")\n    \n    def get_total_time(self) -> float:\n        \"\"\"Get total elapsed time since start.\"\"\"\n        return time.perf_counter() - self.start_time\n\n\nclass Debug:\n    \"\"\"Enhanced structured debug logging system for SearchRouterTool with metrics collection.\"\"\"\n\n    # ANSI color codes\n    _COLORS = {\n        \"RESET\": \"\\x1b[0m\",\n        \"BOLD\": \"\\x1b[1m\",\n        \"DIM\": \"\\x1b[2m\",\n        \"CYAN\": \"\\x1b[96m\",\n        \"GREEN\": \"\\x1b[92m\",\n        \"YELLOW\": \"\\x1b[93m\",\n        \"RED\": \"\\x1b[91m\",\n        \"MAGENTA\": \"\\x1b[95m\",\n        \"BLUE\": \"\\x1b[94m\",\n        \"WHITE\": \"\\x1b[97m\",\n        \"ORANGE\": \"\\x1b[38;5;208m\",\n        \"PURPLE\": \"\\x1b[38;5;129m\",\n    }\n\n    def __init__(self, enabled: bool = False, tool_name: str = \"SearchRouterTool\"):\n        self.enabled = enabled\n        self.tool_name = tool_name\n        self.metrics = DebugMetrics()\n        self._session_id = str(int(time.time()))[-6:]  # Last 6 digits of timestamp\n\n    def _get_timestamp(self) -> str:\n        \"\"\"Get formatted timestamp.\"\"\"\n        return datetime.now().strftime(\"%H:%M:%S.%f\")[:-3]  # Include milliseconds\n\n    def _format_msg(self, category: str, message: str, color: str = \"CYAN\", include_timestamp: bool = True) -> str:\n        \"\"\"Format a debug message with consistent styling and optional timestamp.\"\"\"\n        if not self.enabled:\n            return \"\"\n\n        timestamp = f\"{self._COLORS['DIM']}[{self._get_timestamp()}]{self._COLORS['RESET']} \" if include_timestamp else \"\"\n        prefix = f\"{self._COLORS['MAGENTA']}{self._COLORS['BOLD']}[{self.tool_name}:{self._session_id}]{self._COLORS['RESET']}\"\n        cat_colored = f\"{self._COLORS[color]}{self._COLORS['BOLD']}{category:<12}{self._COLORS['RESET']}\"\n        msg_colored = f\"{self._COLORS[color]}{message}{self._COLORS['RESET']}\"\n\n        return f\"{timestamp}{prefix} {cat_colored}: {msg_colored}\"\n\n    def _log(self, category: str, message: str, color: str = \"CYAN\", track_metric: bool = True) -> None:\n        \"\"\"Internal logging method with optional metrics tracking.\"\"\"\n        if self.enabled:\n            formatted = self._format_msg(category, message, color)\n            if formatted:\n                print(formatted, file=sys.stderr)\n            \n            if track_metric:\n                self.metrics.total_operations += 1\n\n    @contextmanager\n    def timer(self, operation_name: str):\n        \"\"\"Context manager for timing operations.\"\"\"\n        start = time.perf_counter()\n        try:\n            yield\n        finally:\n            duration = time.perf_counter() - start\n            self.metrics.add_operation_time(operation_name, duration)\n            if self.enabled:\n                self._log(\"TIMING\", f\"{operation_name} completed in {duration:.3f}s\", \"ORANGE\", track_metric=False)\n\n    def start_session(self, description: str = \"\") -> None:\n        \"\"\"Start a new debug session.\"\"\"\n        self.metrics = DebugMetrics()  # Reset metrics\n        session_msg = f\"Debug session started\" + (f\": {description}\" if description else \"\")\n        self._log(\"SESSION\", session_msg, \"PURPLE\", track_metric=False)\n        self._log(\"SESSION\", f\"Session ID: {self._session_id}\", \"DIM\", track_metric=False)\n\n    def router(self, message: str) -> None:\n        \"\"\"Log search strategy routing decisions.\"\"\"\n        self._log(\"ROUTER\", message, \"BLUE\")\n\n    def search(self, message: str) -> None:\n        \"\"\"Log search operations.\"\"\"\n        self._log(\"SEARCH\", message, \"GREEN\")\n        self.metrics.search_queries += 1\n\n    def crawl(self, message: str) -> None:\n        \"\"\"Log crawling operations.\"\"\"\n        self._log(\"CRAWL\", message, \"YELLOW\")\n\n    def agent(self, message: str) -> None:\n        \"\"\"Log agentic operations in COMPLETE mode.\"\"\"\n        self._log(\"AGENT\", message, \"MAGENTA\")\n\n    def synthesis(self, message: str) -> None:\n        \"\"\"Log synthesis and summarization.\"\"\"\n        self._log(\"SYNTHESIS\", message, \"WHITE\")\n\n    def error(self, message: str) -> None:\n        \"\"\"Log errors and warnings.\"\"\"\n        self._log(\"ERROR\", message, \"RED\")\n        self.metrics.add_error(message)\n\n    def warning(self, message: str) -> None:\n        \"\"\"Log warnings.\"\"\"\n        self._log(\"WARNING\", message, \"YELLOW\")\n        self.metrics.add_warning(message)\n\n    def flow(self, message: str) -> None:\n        \"\"\"Log general workflow steps.\"\"\"\n        self._log(\"FLOW\", message, \"CYAN\")\n\n    def data(self, label: str, data: Any, truncate: int = 80) -> None:\n        \"\"\"Log data with optional truncation.\"\"\"\n        if not self.enabled:\n            return\n\n        if isinstance(data, str) and len(data) > truncate:\n            data_str = f\"{data[:truncate]}...\"\n            self.metrics.context_truncations += 1\n        else:\n            data_str = str(data)\n\n        self._log(\"DATA\", f\"{label} \u2192 {data_str}\", \"DIM\")\n\n    def query(self, message: str) -> None:\n        \"\"\"Log query refinement operations.\"\"\"\n        self._log(\"QUERY\", message, \"BLUE\")\n\n    def iteration(self, message: str) -> None:\n        \"\"\"Log research iterations in COMPLETE mode.\"\"\"\n        self._log(\"ITERATION\", message, \"CYAN\")\n\n    def llm_call(self, model: str, success: bool = True, duration: float = 0.0) -> None:\n        \"\"\"Track LLM call metrics.\"\"\"\n        self.metrics.llm_calls += 1\n        self.metrics.llm_total_time += duration\n        if not success:\n            self.metrics.llm_failures += 1\n        \n        status = \"\u2713\" if success else \"\u2717\"\n        self._log(\"LLM\", f\"{status} {model} ({duration:.3f}s)\", \"GREEN\" if success else \"RED\")\n\n    def url_metrics(self, found: int = 0, crawled: int = 0, successful: int = 0, failed: int = 0) -> None:\n        \"\"\"Update URL-related metrics.\"\"\"\n        self.metrics.urls_found += found\n        self.metrics.urls_crawled += crawled\n        self.metrics.urls_successful += successful\n        self.metrics.urls_failed += failed\n\n    def content_metrics(self, chars: int, truncated: bool = False) -> None:\n        \"\"\"Update content-related metrics.\"\"\"\n        self.metrics.total_content_chars += chars\n        if truncated:\n            self.metrics.context_truncations += 1\n\n    def report(self, message: str) -> None:\n        \"\"\"Log full debug reports without truncation.\"\"\"\n        if self.enabled:\n            # Split long reports into chunks to avoid terminal truncation\n            lines = message.split(\"\\n\")\n            chunk_size = 25  # Lines per chunk\n\n            for i in range(0, len(lines), chunk_size):\n                chunk = lines[i : i + chunk_size]\n                chunk_text = \"\\n\".join(chunk)\n\n                if i == 0:\n                    # First chunk gets the REPORT prefix\n                    formatted = self._format_msg(\n                        \"REPORT\", f\"Part {(i//chunk_size)+1}:\\n{chunk_text}\", \"WHITE\", include_timestamp=False\n                    )\n                else:\n                    # Subsequent chunks get REPORT-CONT prefix\n                    formatted = self._format_msg(\n                        \"REPORT-CONT\",\n                        f\"Part {(i//chunk_size)+1}:\\n{chunk_text}\",\n                        \"WHITE\",\n                        include_timestamp=False\n                    )\n\n                if formatted:\n                    print(formatted, file=sys.stderr)\n\n    def metrics_summary(self) -> None:\n        \"\"\"Display comprehensive metrics summary at the end of execution.\"\"\"\n        if not self.enabled:\n            return\n        \n        total_time = self.metrics.get_total_time()\n        \n        # Build metrics report\n        report_lines = [\n            \"\",\n            \"\u2550\" * 80,\n            f\"\ud83d\udcca EXECUTION METRICS SUMMARY - {self.tool_name} (Session: {self._session_id})\",\n            \"\u2550\" * 80,\n            \"\",\n            \"\u23f1\ufe0f  TIMING METRICS:\",\n            f\"   Total Execution Time: {total_time:.3f}s\",\n            f\"   Total Operations: {self.metrics.total_operations}\",\n        ]\n        \n        if self.metrics.operation_times:\n            report_lines.append(\"   Operation Breakdown:\")\n            for op, duration in sorted(self.metrics.operation_times.items(), key=lambda x: x[1], reverse=True):\n                percentage = (duration / total_time) * 100 if total_time > 0 else 0\n                report_lines.append(f\"     \u2022 {op}: {duration:.3f}s ({percentage:.1f}%)\")\n        \n        report_lines.extend([\n            \"\",\n            \"\ud83e\udd16 LLM METRICS:\",\n            f\"   Total LLM Calls: {self.metrics.llm_calls}\",\n            f\"   LLM Total Time: {self.metrics.llm_total_time:.3f}s\",\n            f\"   LLM Failures: {self.metrics.llm_failures}\",\n            f\"   Average LLM Time: {(self.metrics.llm_total_time / self.metrics.llm_calls):.3f}s\" if self.metrics.llm_calls > 0 else \"   Average LLM Time: N/A\",\n        ])\n        \n        if self.metrics.search_queries > 0:\n            report_lines.extend([\n                \"\",\n                \"\ud83d\udd0d SEARCH METRICS:\",\n                f\"   Search Queries: {self.metrics.search_queries}\",\n                f\"   URLs Found: {self.metrics.urls_found}\",\n                f\"   URLs Crawled: {self.metrics.urls_crawled}\",\n                f\"   URLs Successful: {self.metrics.urls_successful}\",\n                f\"   URLs Failed: {self.metrics.urls_failed}\",\n                f\"   Success Rate: {(self.metrics.urls_successful / self.metrics.urls_crawled * 100):.1f}%\" if self.metrics.urls_crawled > 0 else \"   Success Rate: N/A\",\n            ])\n        \n        if self.metrics.total_content_chars > 0:\n            report_lines.extend([\n                \"\",\n                \"\ud83d\udcc4 CONTENT METRICS:\",\n                f\"   Total Content Characters: {self.metrics.total_content_chars:,}\",\n                f\"   Context Truncations: {self.metrics.context_truncations}\",\n            ])\n        \n        if self.metrics.errors or self.metrics.warnings:\n            report_lines.extend([\n                \"\",\n                \"\u26a0\ufe0f  ISSUES SUMMARY:\",\n                f\"   Errors: {len(self.metrics.errors)}\",\n                f\"   Warnings: {len(self.metrics.warnings)}\",\n            ])\n            \n            if self.metrics.errors:\n                report_lines.append(\"   Recent Errors:\")\n                for error in self.metrics.errors[-3:]:  # Show last 3 errors\n                    report_lines.append(f\"     \u2022 {error}\")\n            \n            if self.metrics.warnings:\n                report_lines.append(\"   Recent Warnings:\")\n                for warning in self.metrics.warnings[-3:]:  # Show last 3 warnings\n                    report_lines.append(f\"     \u2022 {warning}\")\n        \n        report_lines.extend([\n            \"\",\n            \"\u2550\" * 80,\n            \"\"\n        ])\n        \n        # Print the metrics report\n        metrics_report = \"\\n\".join(report_lines)\n        formatted = self._format_msg(\"METRICS\", metrics_report, \"PURPLE\", include_timestamp=False)\n        if formatted:\n            print(formatted, file=sys.stderr)\n\n\n# Legacy compatibility - will be replaced\ndef _debug(msg: str) -> None:\n    \"\"\"Legacy debug function - use Debug class instead.\"\"\"\n    print(\n        f\"{Debug._COLORS['MAGENTA']}{Debug._COLORS['BOLD']}[SearchRouterTool]{Debug._COLORS['RESET']}{Debug._COLORS['CYAN']} {msg}{Debug._COLORS['RESET']}\",\n        file=sys.stderr,\n    )\n\n\n# \u2500\u2500\u2500 Constants & Helpers \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nURL_RE = re.compile(r\"https?://\\S+\")\n\n\ndef _get_text_from_message(message_content: Any) -> str:\n    \"\"\"Extracts only the text part of a message, ignoring image data URLs.\"\"\"\n    if isinstance(message_content, list):\n        text_parts = []\n        for part in message_content:\n            if isinstance(part, dict) and part.get(\"type\") == \"text\":\n                text_parts.append(part.get(\"text\", \"\"))\n        return \" \".join(text_parts)\n    elif isinstance(message_content, str):\n        return message_content\n    return \"\"\n\n\nasync def generate_with_retry(\n    max_retries: int = 3, delay: int = 3, debug: Debug = None, **kwargs: Any\n) -> Dict[str, Any]:\n    \"\"\"\n    A wrapper for generate_chat_completion that includes a retry mechanism with exponential backoff.\n    \"\"\"\n    def _normalize_llm_response(res: Any) -> Dict[str, Any]:\n        # Already a dict\n        if isinstance(res, dict):\n            return res\n        \n        # Debug what we actually got\n        if debug:\n            debug.error(f\"LLM response normalization needed. Type: {type(res)}, Dir: {[attr for attr in dir(res) if not attr.startswith('_')]}\")\n        \n        # If it's a JSONResponse (Starlette/FastAPI), extract the body and parse\n        try:\n            if hasattr(res, 'body') and hasattr(res, 'status_code'):\n                import json\n                body_bytes = res.body\n                if isinstance(body_bytes, bytes):\n                    parsed = json.loads(body_bytes.decode('utf-8'))\n                    if debug:\n                        debug.flow(f\"Successfully parsed JSONResponse body\")\n                        debug.data(\"Full parsed response\", str(parsed)[:500] + \"...\" if len(str(parsed)) > 500 else str(parsed))\n                        debug.data(\"Response keys\", list(parsed.keys()) if isinstance(parsed, dict) else \"Not a dict\")\n                    return parsed\n                elif isinstance(body_bytes, str):\n                    parsed = json.loads(body_bytes)\n                    if debug:\n                        debug.flow(f\"Successfully parsed JSONResponse string body\")\n                        debug.data(\"Full parsed response\", str(parsed)[:500] + \"...\" if len(str(parsed)) > 500 else str(parsed))\n                        debug.data(\"Response keys\", list(parsed.keys()) if isinstance(parsed, dict) else \"Not a dict\")\n                    return parsed\n        except Exception as e:\n            if debug:\n                debug.error(f\"Failed to parse JSONResponse body: {e}\")\n        \n        # Try to call render() method if available (for Response objects)\n        try:\n            if hasattr(res, 'render'):\n                import json\n                rendered = res.render(None)  # Pass None as scope if not needed\n                if isinstance(rendered, bytes):\n                    parsed = json.loads(rendered.decode('utf-8'))\n                    if debug:\n                        debug.flow(f\"Successfully parsed rendered response\")\n                    return parsed\n        except Exception as e:\n            if debug:\n                debug.error(f\"Failed to render and parse response: {e}\")\n        \n        # Check if it's already JSON-like but not a dict\n        try:\n            if hasattr(res, '__dict__'):\n                return res.__dict__\n        except Exception:\n            pass\n            \n        # Last resort: if it has a dict-like interface\n        try:\n            return dict(res)  # may raise\n        except Exception as e:\n            if debug:\n                debug.error(f\"Failed to convert to dict: {e}\")\n            raise TypeError(f\"LLM response is not a dict and could not be normalized. Type: {type(res)}, Value: {str(res)[:200]}\")\n\n    model_name = kwargs.get('form_data', {}).get('model', 'unknown')\n    last_exception = None\n    \n    for attempt in range(max_retries):\n        start_time = time.perf_counter()\n        try:\n            raw = await generate_chat_completion(**kwargs)\n            result = _normalize_llm_response(raw)\n            duration = time.perf_counter() - start_time\n            \n            if debug:\n                debug.llm_call(model_name, success=True, duration=duration)\n                if attempt > 0:\n                    debug.flow(f\"LLM call succeeded on attempt {attempt + 1}\")\n            return result\n        except Exception as e:\n            duration = time.perf_counter() - start_time\n            last_exception = e\n            \n            if debug:\n                debug.llm_call(model_name, success=False, duration=duration)\n                debug.error(\n                    f\"LLM call failed on attempt {attempt + 1}/{max_retries}: {str(e)[:100]}...\"\n                )\n\n            # Don't wait on the last attempt\n            if attempt < max_retries - 1:\n                # Exponential backoff with jitter\n                wait_time = delay * (2**attempt) + (attempt * 0.5)\n                await asyncio.sleep(wait_time)\n\n    if debug:\n        debug.error(\n            f\"LLM call failed after {max_retries} retries. Last error: {str(last_exception)[:100]}...\"\n        )\n    raise last_exception\n\n\nasync def generate_with_parsing_retry(\n    max_retries: int = 3, delay: int = 3, debug: Debug = None, \n    expected_keys: List[str] = None, **kwargs: Any\n) -> Dict[str, Any]:\n    \"\"\"\n    A wrapper that combines generate_with_retry with parsing retry logic.\n    Retries both API failures and response parsing failures.\n    \"\"\"\n    last_exception = None\n    \n    for attempt in range(max_retries):\n        try:\n            # First, try the API call with retry\n            result = await generate_with_retry(max_retries=max_retries, delay=delay, debug=debug, **kwargs)\n            \n            # Then validate the response format\n            if expected_keys:\n                if isinstance(result, dict):\n                    # Check if any expected key exists\n                    if any(key in result for key in expected_keys):\n                        return result\n                    else:\n                        # Log the parsing issue and retry\n                        if debug:\n                            debug.warning(f\"Response missing expected keys {expected_keys}. Got keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}. Attempt {attempt + 1}/{max_retries}\")\n                        raise ValueError(f\"Response missing expected keys {expected_keys}. Keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}\")\n                else:\n                    if debug:\n                        debug.warning(f\"Response is not a dict. Type: {type(result)}. Attempt {attempt + 1}/{max_retries}\")\n                    raise ValueError(f\"Response is not a dict. Type: {type(result)}\")\n            else:\n                # No specific validation needed, return result\n                return result\n                \n        except Exception as e:\n            last_exception = e\n            \n            if debug:\n                debug.error(f\"Generate with parsing retry failed on attempt {attempt + 1}/{max_retries}: {str(e)[:100]}...\")\n            \n            # Don't wait on the last attempt\n            if attempt < max_retries - 1:\n                # Use same exponential backoff as generate_with_retry\n                wait_time = delay * (2**attempt) + (attempt * 0.5)\n                if debug:\n                    debug.flow(f\"Waiting {wait_time:.1f}s before retry attempt {attempt + 2}\")\n                await asyncio.sleep(wait_time)\n    \n    if debug:\n        debug.error(f\"Generate with parsing retry failed after {max_retries} retries. Last error: {str(last_exception)[:100]}...\")\n    raise last_exception\n\n\n# Debug Report Dataclasses\n\n\n@dataclass\nclass QuickDebugReport:\n    \"\"\"Enhanced structured report for debugging the STANDARD search process.\"\"\"\n\n    initial_query: str = \"\"\n    refined_query: str = \"\"\n    urls_found: List[str] = field(default_factory=list)\n    urls_crawled: List[str] = field(default_factory=list)\n    urls_successful: List[str] = field(default_factory=list)\n    urls_failed: List[str] = field(default_factory=list)\n    final_prompt: str = \"\"\n    final_output: str = \"\"\n\n    # Valve settings for comparison\n    valve_urls_to_search: int = 0\n    valve_queries_to_crawl: int = 0\n    valve_max_context_chars: int = 0\n\n    # Actual metrics\n    context_length: int = 0\n    was_truncated: bool = False\n\n    def format_report(self) -> str:\n        report_parts = [\n            \"\\n\\n\" + \"=\" * 30 + \" STANDARD SEARCH DEBUG REPORT \" + \"=\" * 30,\n            f\"INITIAL USER QUERY: {self.initial_query}\",\n            f\"REFINED SEARCH QUERY: {self.refined_query}\",\n            \"\",\n            \"\u2500\u2500\u2500 VALVE SETTINGS vs ACTUAL RESULTS \u2500\u2500\u2500\",\n            f\"URLs to Search (valve): {self.valve_urls_to_search} | Found: {len(self.urls_found)}\",\n            f\"URLs to Crawl (valve): {self.valve_queries_to_crawl} | Attempted: {len(self.urls_crawled)} | Successful: {len(self.urls_successful)}\",\n            f\"Max Context Chars (valve): {self.valve_max_context_chars} | Used: {self.context_length} {'(TRUNCATED)' if self.was_truncated else ''}\",\n            \"\",\n            \"\u2500\u2500\u2500 SEARCH RESULTS \u2500\u2500\u2500\",\n            f\"URLs Found ({len(self.urls_found)}):\",\n        ]\n\n        for i, url in enumerate(self.urls_found, 1):\n            report_parts.append(f\"  {i}. {url}\")\n\n        report_parts.extend(\n            [\n                \"\",\n                \"\u2500\u2500\u2500 CRAWL RESULTS \u2500\u2500\u2500\",\n                f\"Successfully Crawled ({len(self.urls_successful)}):\",\n            ]\n        )\n\n        for i, url in enumerate(self.urls_successful, 1):\n            report_parts.append(f\"  \u2713 {i}. {url}\")\n\n        if self.urls_failed:\n            report_parts.extend(\n                [\n                    f\"Failed to Crawl ({len(self.urls_failed)}):\",\n                ]\n            )\n            for i, url in enumerate(self.urls_failed, 1):\n                report_parts.append(f\"  \u2717 {i}. {url}\")\n\n        report_parts.extend(\n            [\n                \"\",\n                \"\u2500\u2500\u2500 SYNTHESIS \u2500\u2500\u2500\",\n                f\"Final Output Preview:\\n{self.final_output[:400]}{'...' if len(self.final_output) > 400 else ''}\",\n                \"\",\n                \"=\" * 91 + \"\\n\",\n            ]\n        )\n        return \"\\n\".join(report_parts)\n\n\n@dataclass\nclass CompleteDebugReport:\n    \"\"\"Enhanced structured report for debugging the COMPLETE search process.\"\"\"\n\n    initial_user_query: str = \"\"\n    refined_initial_query: str = \"\"\n    iterations: List[Dict[str, Any]] = field(default_factory=list)\n    final_decision: str = \"\"\n    final_payload: str = \"\"\n    final_output: str = \"\"\n\n    # Valve settings for comparison\n    valve_urls_per_query: int = 0\n    valve_queries_to_crawl: int = 0\n    valve_queries_to_generate: int = 0\n    valve_max_iterations: int = 0\n\n    # Total metrics\n    total_sources_found: int = 0\n\n    def add_iteration(\n        self,\n        iteration_number: int,\n        continue_decision: str,\n        reasoning_notes: str,\n        generated_queries: List[str],\n    ):\n        self.iterations.append(\n            {\n                \"iteration\": iteration_number,\n                \"continue_decision\": continue_decision,\n                \"reasoning_notes\": reasoning_notes,\n                \"generated_queries\": generated_queries,\n                \"searches\": [],\n            }\n        )\n\n    def add_search_to_iteration(\n        self, iteration_number: int, query: str, crawled_urls: List[str]\n    ):\n        for iter_data in self.iterations:\n            if iter_data[\"iteration\"] == iteration_number:\n                iter_data[\"searches\"].append(\n                    {\"query\": query, \"crawled_urls\": crawled_urls}\n                )\n                return\n\n    def format_report(self) -> str:\n        # Calculate total metrics\n        total_queries_executed = sum(\n            len(iter_data.get(\"searches\", [])) for iter_data in self.iterations\n        )\n        total_sources_crawled = sum(\n            len(search.get(\"crawled_urls\", []))\n            for iter_data in self.iterations\n            for search in iter_data.get(\"searches\", [])\n        )\n        actual_iterations = len(\n            [i for i in self.iterations if i.get(\"iteration\", 0) > 0]\n        )\n\n        report_parts = [\n            \"\\n\\n\" + \"=\" * 30 + \" COMPLETE SEARCH DEBUG REPORT \" + \"=\" * 30,\n            f\"INITIAL USER QUERY: {self.initial_user_query}\",\n            f\"REFINED SEARCH QUERY: {self.refined_initial_query}\",\n            \"\",\n            \"\u2500\u2500\u2500 VALVE SETTINGS vs ACTUAL RESULTS \u2500\u2500\u2500\",\n            f\"Max Iterations (valve): {self.valve_max_iterations} | Executed: {actual_iterations}\",\n            f\"URLs per Query (valve): {self.valve_urls_per_query} | Queries to Crawl (valve): {self.valve_queries_to_crawl}\",\n            f\"Queries to Generate (valve): {self.valve_queries_to_generate}\",\n            f\"Total Queries Executed: {total_queries_executed} | Total Sources Gathered: {total_sources_crawled}\",\n            f\"Unique Sources in Notepad: {self.total_sources_found}\",\n            \"\",\n            \"\u2500\u2500\u2500 RESEARCH ITERATIONS \u2500\u2500\u2500\",\n        ]\n\n        for iter_data in self.iterations:\n            iteration_num = iter_data.get(\"iteration\", \"N/A\")\n\n            if iteration_num == 0:\n                report_parts.append(\"INITIAL SEARCH:\")\n                if iter_data.get(\"searches\"):\n                    initial_search = iter_data[\"searches\"][0]\n                    query = initial_search.get(\"query\", \"N/A\")\n                    crawled_urls = initial_search.get(\"crawled_urls\", [])\n                    report_parts.append(f'  Query: \"{query}\"')\n                    report_parts.append(f\"  Sources Found: {len(crawled_urls)}\")\n                    for i, url in enumerate(crawled_urls, 1):\n                        report_parts.append(f\"    {i}. {url}\")\n            else:\n                decision = iter_data.get(\"continue_decision\", \"N/A\")\n                notes = iter_data.get(\"reasoning_notes\", \"\")\n                queries = iter_data.get(\"generated_queries\", [])\n\n                report_parts.append(f\"ITERATION {iteration_num}:\")\n                report_parts.append(f\"  Decision: {decision}\")\n\n                if queries:\n                    report_parts.append(f\"  Generated Queries ({len(queries)}):\")\n                    for i, query in enumerate(queries, 1):\n                        report_parts.append(f\"    {i}. {query}\")\n\n                for search_data in iter_data.get(\"searches\", []):\n                    query = search_data.get(\"query\", \"N/A\")\n                    crawled_urls = search_data.get(\"crawled_urls\", [])\n                    report_parts.append(\n                        f\"  Search: \\\"{query[:60]}{'...' if len(query) > 60 else ''}\\\"\"\n                    )\n                    report_parts.append(f\"    Sources: {len(crawled_urls)}\")\n                    for i, url in enumerate(crawled_urls, 1):\n                        report_parts.append(f\"      {i}. {url}\")\n\n            report_parts.append(\"\")\n\n        report_parts.extend(\n            [\n                \"\u2500\u2500\u2500 FINAL SYNTHESIS \u2500\u2500\u2500\",\n                f\"Synthesis Decision: {self.final_decision}\",\n                f\"Final Output Preview:\\n{self.final_output[:400]}{'...' if len(self.final_output) > 400 else ''}\",\n                \"\",\n                \"=\" * 91 + \"\\n\",\n            ]\n        )\n        return \"\\n\".join(report_parts)\n\n\n# Valves and core functionality\nclass ToolsInternal:\n\n    class Valves(BaseModel):\n        exa_api_key: str = Field(default=\"\", description=\"Your Exa API key.\")\n        router_model: str = Field(\n            default=\"gpt-4o-mini\",\n            description=\"LLM for the initial CRAWL/STANDARD/COMPLETE decision.\",\n        )\n        quick_search_model: str = Field(\n            default=\"gpt-4o-mini\",\n            description=\"Single 'helper' model for all tasks in the STANDARD path (refining, summarizing).\",\n        )\n        complete_agent_model: str = Field(\n            default=\"gpt-4-turbo\",\n            description=\"The 'smart' model for all agentic steps in the COMPLETE path (refining, deciding, query generation).\",\n        )\n        complete_summarizer_model: str = Field(\n            default=\"gpt-4-turbo\",\n            description=\"Dedicated high-quality model for the final summary in the COMPLETE path.\",\n        )\n        quick_urls_to_search: int = Field(\n            default=5, description=\"Number of URLs to fetch for STANDARD search.\"\n        )\n        quick_queries_to_crawl: int = Field(\n            default=3, description=\"Number of top URLs to crawl for STANDARD search.\"\n        )\n        quick_max_context_chars: int = Field(\n            default=8000,\n            description=\"Maximum total characters of context to feed to the STANDARD search summarizer.\",\n        )\n        complete_urls_to_search_per_query: int = Field(\n            default=5,\n            description=\"Number of URLs to fetch for each targeted query in COMPLETE search.\",\n        )\n        complete_queries_to_crawl: int = Field(\n            default=3,\n            description=\"Number of top URLs to crawl for each targeted query in COMPLETE search.\",\n        )\n        complete_queries_to_generate: int = Field(\n            default=3,\n            description=\"Number of new targeted queries to generate per iteration.\",\n        )\n        complete_max_search_iterations: int = Field(\n            default=2, description=\"Maximum number of research loops for the agent.\"\n        )\n        # Custom Standard Search Settings\n        use_custom_standard_search: bool = Field(\n            default=False,\n            description=\"Use the new agentic custom search for STANDARD mode instead of the default pipeline.\",\n        )\n        custom_search_agent_model: str = Field(\n            default=\"gpt-4o-mini\",\n            description=\"Agent model for extraction, evaluation, and query generation in custom standard search.\",\n        )\n        custom_search_urls_per_query: int = Field(\n            default=5,\n            description=\"Number of URLs to find per search query in custom standard search.\",\n        )\n        custom_search_urls_to_crawl: int = Field(\n            default=3,\n            description=\"Number of top URLs to crawl per search query in custom standard search.\",\n        )\n        custom_search_max_iterations: int = Field(\n            default=3,\n            description=\"Maximum number of search iterations before returning results.\",\n        )\n        custom_search_queries_per_iteration: int = Field(\n            default=2,\n            description=\"Number of search queries to generate per iteration.\",\n        )\n        debug_enabled: bool = Field(\n            default=False,\n            description=\"Enable detailed debug logging for troubleshooting search operations.\",\n        )\n        show_sources: bool = Field(\n            default=False,\n            description=\"If true, return show_source=True so the UI can display sources. Prompts instruct the LLM not to include explicit citations in the text unless asked.\",\n        )\n\n    def __init__(self) -> None:\n        self.valves = self.Valves()\n        self.debug = Debug(enabled=False)  # Will be updated when valves change\n        self._exa: Optional[Exa] = None\n        self._query_cache: Dict[str, Any] = {}  # Simple query caching\n        self._cache_max_size = 100  # Limit cache size\n        self._active_sessions: Dict[str, asyncio.Lock] = {}  # Session concurrency control\n        self._session_lock = asyncio.Lock()  # Lock for managing session locks\n        self._last_error: Optional[str] = None  # Track the most recent error for user feedback\n\n    def _exa_client(self) -> Exa:\n        if self._exa is None:\n            if Exa is None:\n                raise RuntimeError(\n                    \"exa_py not installed. Please install with: pip install exa_py\"\n                )\n            key = self.valves.exa_api_key or os.getenv(\"EXA_API_KEY\")\n            if not key:\n                raise RuntimeError(\n                    \"Exa API key missing. Please set exa_api_key in valves or EXA_API_KEY environment variable\"\n                )\n            try:\n                self._exa = Exa(key)\n                self.debug.flow(\"\ud83d\udd11 Exa client initialised successfully\")\n            except Exception as e:\n                self.debug.error(f\"Failed to initialize Exa client: {e}\")\n                raise RuntimeError(f\"Failed to initialize Exa client: {e}\")\n        return self._exa\n\n    async def _safe_exa_search(\n        self, query: str, num_results: int, debug_context: str = \"\"\n    ) -> List[Any]:\n        \"\"\"Safely perform Exa search with error handling, caching, and latency metrics.\"\"\"\n        # Simple cache key based on query and num_results\n        cache_key = f\"{query}:{num_results}\"\n\n        # Check cache first\n        if cache_key in self._query_cache:\n            self.debug.search(f\"Cache hit for {debug_context}: using cached results\")\n            return self._query_cache[cache_key]\n\n        with self.debug.timer(f\"exa_search_{debug_context}\"):\n            try:\n                exa = self._exa_client()\n                search_data = await asyncio.to_thread(\n                    exa.search, query, num_results=num_results\n                )\n                results = search_data.results\n\n                # Cache the results (with size limit)\n                if len(self._query_cache) >= self._cache_max_size:\n                    # Remove oldest entry (simple FIFO)\n                    oldest_key = next(iter(self._query_cache))\n                    del self._query_cache[oldest_key]\n\n                self._query_cache[cache_key] = results\n                self.debug.search(\n                    f\"Exa search successful for {debug_context}: {len(results)} results (cached)\"\n                )\n                self.debug.url_metrics(found=len(results))\n                return results\n            except Exception as e:\n                error_msg = str(e)\n                self._last_error = error_msg  # Store for user-facing error messages\n                self.debug.error(\n                    f\"Exa search failed for {debug_context}: {error_msg[:100]}...\"\n                )\n                return []\n\n    async def _safe_exa_crawl(\n        self, ids_or_urls: List[str], debug_context: str = \"\"\n    ) -> List[Any]:\n        \"\"\"Safely perform Exa content crawling with error handling, chunking, concurrency, and latency metrics.\"\"\"\n        if not ids_or_urls:\n            return []\n\n        with self.debug.timer(f\"exa_crawl_{debug_context}\"):\n            try:\n                exa = self._exa_client()\n                # Chunk inputs to avoid oversized requests; run chunks concurrently\n                chunk_size = 10\n                chunks: List[List[str]] = [ids_or_urls[i:i+chunk_size] for i in range(0, len(ids_or_urls), chunk_size)]\n\n                async def fetch_chunk(chunk: List[str]):\n                    return await asyncio.to_thread(exa.get_contents, chunk)\n\n                results_list = await asyncio.gather(*[fetch_chunk(c) for c in chunks], return_exceptions=True)\n\n                combined = []\n                total_requested = len(ids_or_urls)\n                total_success = 0\n                total_content_chars = 0\n                \n                for item in results_list:\n                    if isinstance(item, Exception):\n                        error_msg = str(item)\n                        self._last_error = error_msg  # Store for user-facing error messages\n                        self.debug.error(f\"Exa crawl chunk failed for {debug_context}: {error_msg[:100]}...\")\n                        continue\n                    # item.results is expected from Exa\n                    combined.extend(item.results)\n                    total_success += len(item.results)\n                    # Track content size\n                    for result in item.results:\n                        total_content_chars += len(getattr(result, 'text', ''))\n\n                failed_count = total_requested - total_success\n                \n                if total_success < total_requested:\n                    self.debug.error(\n                        f\"Exa crawl partial failure for {debug_context}: {total_success}/{total_requested} succeeded\"\n                    )\n                else:\n                    self.debug.crawl(\n                        f\"Exa crawl successful for {debug_context}: {total_success} sources\"\n                    )\n\n                # Update metrics\n                self.debug.url_metrics(crawled=total_requested, successful=total_success, failed=failed_count)\n                self.debug.content_metrics(total_content_chars)\n                \n                return combined\n            except Exception as e:\n                error_msg = str(e)\n                self._last_error = error_msg  # Store for user-facing error messages\n                self.debug.error(f\"Exa crawl failed for {debug_context}: {error_msg[:100]}...\")\n                self.debug.url_metrics(crawled=len(ids_or_urls), failed=len(ids_or_urls))\n                return []\n\n    def _extract_section(self, text: str, section_marker: str) -> str:\n        \"\"\"\n        Extract multi-line content from a section that starts with section_marker.\n        Stops when it hits another known section marker or end of text.\n        \"\"\"\n        known_markers = [\"EXTRACTED_INFO:\", \"EVALUATION:\", \"DECISION:\", \"NEXT_QUERIES:\", \"STATUS_SUMMARY:\", \"RESEARCH_SUMMARY:\", \"KEY_POINTS:\"]\n        lines = text.split(\"\\n\")\n        result_lines = []\n        capturing = False\n\n        for line in lines:\n            line_upper = line.upper().strip()\n\n            # Check if this line starts a section we're looking for\n            if section_marker.upper().rstrip(\":\") in line_upper:\n                capturing = True\n                # Get content after the marker on the same line\n                if \":\" in line:\n                    after_marker = line.split(\":\", 1)[1].strip()\n                    if after_marker:\n                        result_lines.append(after_marker)\n                continue\n\n            # If capturing, check if we hit another section marker\n            if capturing:\n                is_new_section = any(m.upper().rstrip(\":\") in line_upper for m in known_markers if m.upper() != section_marker.upper())\n                if is_new_section:\n                    break\n                result_lines.append(line)\n\n        result = \"\\n\".join(result_lines).strip()\n\n        # Fallback: if nothing captured, try regex between markers\n        if not result:\n            import re\n            pattern = rf\"{re.escape(section_marker)}\\s*(.*?)(?=(?:EVALUATION|DECISION|NEXT_QUERIES|STATUS_SUMMARY|KEY_POINTS|RESEARCH_SUMMARY):|$)\"\n            match = re.search(pattern, text, re.IGNORECASE | re.DOTALL)\n            if match:\n                result = match.group(1).strip()\n\n        return result\n\n    async def _extract_with_correction(\n        self,\n        *,\n        request: Any,\n        user_obj: Any,\n        model: str,\n        original_text: str,\n        prefix: str,\n        validate: Callable[[str], bool],\n        correction_instructions: str,\n        max_attempts: int = 2,\n    ) -> str:\n        \"\"\"\n        Extracts the substring following a required prefix from LLM output. If missing or invalid,\n        performs up to `max_attempts` corrective re-prompts to coerce the model to output the correct format.\n\n        Returns the best-effort extracted value. Callers should still defensively validate the return value.\n        \"\"\"\n        def _extract(text: str) -> str:\n            for line in text.splitlines():\n                if line.strip().startswith(prefix):\n                    return line.split(prefix, 1)[1].strip()\n            return \"\"\n\n        # First, try to extract from the original text\n        extracted = _extract(original_text)\n        if not extracted:\n            # As a fallback, if there's only one non-empty line, use it\n            lines = [ln.strip() for ln in original_text.splitlines() if ln.strip()]\n            if len(lines) == 1:\n                extracted = lines[0]\n\n        if extracted and validate(extracted):\n            return extracted\n\n        # Attempt corrective re-prompts\n        attempt = 0\n        while attempt < max_attempts:\n            attempt += 1\n            self.debug.warning(\n                f\"Output missing/invalid '{prefix}' prefix. Attempting corrective re-prompt ({attempt}/{max_attempts}).\"\n            )\n            payload = {\n                \"model\": model,\n                \"messages\": [\n                    {\n                        \"role\": \"system\",\n                        \"content\": correction_instructions,\n                    },\n                    {\n                        \"role\": \"user\",\n                        \"content\": f\"Original response to fix:\\n{original_text}\",\n                    },\n                ],\n                \"stream\": False,\n            }\n\n            try:\n                fixed = await generate_with_retry(\n                    request=request, form_data=payload, user=user_obj, debug=self.debug\n                )\n                fixed_text = fixed.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n                self.debug.data(\"Correction full response\", fixed_text, truncate=200)\n\n                extracted = _extract(fixed_text) or fixed_text.strip()\n                if extracted and validate(extracted):\n                    return extracted\n            except Exception as e:\n                self.debug.error(f\"Corrective re-prompt failed: {e}\")\n\n        # Last resort: return a trimmed original if it passes, else empty string\n        fallback = original_text.strip()\n        return fallback if validate(fallback) else \"\"\n\n    # Main\n    async def _get_session_lock(self, user_id: str, query_hash: str) -> asyncio.Lock:\n        \"\"\"Get or create a session lock for concurrent call protection.\"\"\"\n        session_key = f\"{user_id}_{query_hash}\"\n        \n        async with self._session_lock:\n            if session_key not in self._active_sessions:\n                self._active_sessions[session_key] = asyncio.Lock()\n            return self._active_sessions[session_key]\n    \n    async def _cleanup_session_lock(self, user_id: str, query_hash: str) -> None:\n        \"\"\"Clean up session lock after completion.\"\"\"\n        session_key = f\"{user_id}_{query_hash}\"\n        \n        async with self._session_lock:\n            if session_key in self._active_sessions:\n                del self._active_sessions[session_key]\n\n    async def routed_search(\n        self,\n        query: str,\n        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,\n        __request__: Optional[Any] = None,\n        __user__: Optional[Dict] = None,\n        __messages__: Optional[List[Dict]] = None,\n        image_context: Optional[str] = None,\n    ) -> dict:\n        # Guard: some environments may not have show_sources in valves\n        show_sources = bool(getattr(self.valves, \"show_sources\", False))\n        # Check if Exa is available first\n        if not EXA_AVAILABLE:\n            error_msg = \"\u274c Search tool unavailable: exa_py module not installed. Please install with: pip install exa_py\"\n            self.debug.error(error_msg)\n            return {\n                \"content\": error_msg,\n                \"show_source\": show_sources,\n            }\n\n        # Generate session identifiers for concurrency control\n        user_id = __user__.get(\"id\", \"unknown\") if __user__ else \"unknown\"\n        query_hash = str(hash(query))[-8:]  # Use last 8 chars of query hash\n        \n        # Get session lock to prevent concurrent calls\n        session_lock = await self._get_session_lock(user_id, query_hash)\n        \n        # Check if another instance is already running for this user/query combo\n        if session_lock.locked():\n            self.debug.flow(f\"Concurrent call detected for user {user_id}, query hash {query_hash}\")\n            return {\n                \"content\": \"\u26a0\ufe0f A search is already in progress for this query. Please wait for it to complete before starting a new search.\",\n                \"show_source\": show_sources,\n            }\n        \n        # Acquire the lock for this session\n        async with session_lock:\n            # Update debug state based on current valve setting\n            self.debug.enabled = self.valves.debug_enabled\n            if self.debug.enabled:\n                self.debug.start_session(f\"Query: {query[:50]}...\")\n            self.debug.flow(\"Starting SearchRouterTool processing\")\n            \n            try:\n                return await self._execute_search(\n                    query, __event_emitter__, __request__, __user__, __messages__, image_context, show_sources\n                )\n            finally:\n                # Clean up the session lock\n                await self._cleanup_session_lock(user_id, query_hash)\n                # Safety: ensure any transient status is cleared at the end\n                if __event_emitter__:\n                    try:\n                        await __event_emitter__({\n                            \"type\": \"status\",\n                            \"data\": {\"description\": \"\", \"done\": True},\n                        })\n                    except Exception:\n                        pass\n    \n    async def _execute_search(\n        self,\n        query: str,\n        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,\n        __request__: Optional[Any] = None,\n        __user__: Optional[Dict] = None,\n        __messages__: Optional[List[Dict]] = None,\n        image_context: Optional[str] = None,\n        show_sources: bool = False,\n    ) -> dict:\n\n        async def _status(desc: str, done: bool = False) -> None:\n            if __event_emitter__:\n                await __event_emitter__(\n                    {\"type\": \"status\", \"data\": {\"description\": desc, \"done\": done}}\n                )\n\n        # Add debug info about the tool being called\n        self.debug.flow(f\"ExaSearch tool called with query: {query[:100]}...\")\n        if __user__:\n            self.debug.data(\"User ID\", __user__.get(\"id\", \"unknown\"))\n        if __messages__:\n            self.debug.data(\"Message count\", len(__messages__))\n\n        messages = __messages__ or []\n        last_user_message = get_last_user_message(messages)\n        if not last_user_message:\n            self.debug.error(\"Could not find a user message to process\")\n            return {\n                \"content\": \"Could not find a user message to process. Please try again.\",\n                \"show_source\": show_sources,\n            }\n\n        self.debug.data(\"User query\", query)\n        self.debug.data(\"Last user message\", last_user_message)\n\n        # Build conversation history snippet for context\n        history_messages = messages[-6:-1]\n        convo_snippet_parts = []\n        for m in history_messages:\n            text_content = _get_text_from_message(m.get(\"content\", \"\"))\n            role = m.get(\"role\", \"\").upper()\n            convo_snippet_parts.append(f\"{role}: {text_content!r}\")\n        convo_snippet = \"\\n\".join(convo_snippet_parts)\n\n        # Include image context if provided by autotoo\n        if image_context:\n            self.debug.flow(\"Image context provided by autotoo, enhancing search\")\n            convo_snippet += f\"\\n\\nIMAGE CONTEXT: {image_context}\"\n\n        # The definitive query for the router now includes history\n        router_query = f\"Conversation History:\\n{convo_snippet}\\n\\nLatest User Query:\\n'{last_user_message}'\"\n\n        user_obj = Users.get_user_by_id(__user__[\"id\"]) if __user__ else None\n        self.debug.router(f\"Router triggered with full query context\")\n        self.debug.data(\"Router query\", router_query, truncate=150)\n        await _status(\"Deciding search strategy\u2026\")\n\n        # Check for per-call override from upstream router to skip internal strategy LLM\n        override_decision = \"\"\n        if __messages__:\n            try:\n                for m in __messages__:\n                    if m.get(\"role\") == \"system\" and isinstance(m.get(\"content\"), str):\n                        content = m.get(\"content\", \"\")\n                        if \"[EXA_SEARCH_MODE]\" in content:\n                            # Expected formats:\n                            # [EXA_SEARCH_MODE] STANDARD\n                            # [EXA_SEARCH_MODE] CRAWL\n                            # [EXA_SEARCH_MODE] COMPLETE\n                            mode = (\n                                content.split(\"[EXA_SEARCH_MODE]\", 1)[1]\n                                .strip()\n                                .split()[0]\n                                .upper()\n                            )\n                            if mode in {\"CRAWL\", \"STANDARD\", \"COMPLETE\"}:\n                                override_decision = mode\n                                self.debug.router(\n                                    f\"Override strategy detected via system message \u2192 {override_decision}\"\n                                )\n                                break\n            except Exception as _:\n                # Swallow parsing errors to avoid breaking normal flow\n                pass\n\n        if override_decision:\n            decision = override_decision\n        else:\n            router_payload = {\n                \"model\": self.valves.router_model,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": SEARCH_STRATEGY_ROUTER_PROMPT_TEMPLATE},\n                    {\"role\": \"user\", \"content\": router_query},  # Use the full context query\n                ],\n                \"stream\": False,\n            }\n            try:\n                res = await generate_with_retry(\n                    request=__request__,\n                    form_data=router_payload,\n                    user=user_obj,\n                    debug=self.debug,\n                )\n                llm_response_text = res[\"choices\"][0][\"message\"][\"content\"]\n                self.debug.data(\"Router full response\", llm_response_text, truncate=200)\n\n                # Extract the decision using a robust prefixed-line correction helper\n                decision = await self._extract_with_correction(\n                    request=__request__,\n                    user_obj=user_obj,\n                    model=self.valves.router_model,\n                    original_text=llm_response_text,\n                    prefix=\"ANSWER:\",\n                    validate=lambda s: s in {\"CRAWL\", \"STANDARD\", \"COMPLETE\"},\n                    correction_instructions=(\n                        \"Return exactly one line with the routing decision in the format 'ANSWER: CRAWL' or 'ANSWER: STANDARD' or 'ANSWER: COMPLETE'. \"\n                        \"Do not include any other text.\"\n                    ),\n                )\n\n                if decision not in {\"CRAWL\", \"STANDARD\", \"COMPLETE\"}:\n                    self.debug.router(\n                        \"Could not parse router decision even after correction. Defaulting to STANDARD.\"\n                    )\n                    decision = \"STANDARD\"\n\n            except Exception as exc:\n                self.debug.error(\n                    f\"Router LLM failed after retries: {exc}. Defaulting to STANDARD.\"\n                )\n                decision = \"STANDARD\"\n\n        self.debug.router(f\"Router decision \u2192 {decision}\")\n        exa = self._exa_client()\n\n        # Mode 1 - Crawl\n        if decision == \"CRAWL\":\n            self._last_error = None  # Clear any previous errors\n            urls = URL_RE.findall(last_user_message)\n            if not urls:\n                return {\n                    \"content\": \"You requested a crawl, but I could not find a URL in your message. Please provide a valid URL.\",\n                    \"show_source\": show_sources,\n                }\n\n            url_to_crawl = urls[0]\n            await _status(\"Reading content from URL...\")\n            self.debug.crawl(f\"Executing CRAWL on: {url_to_crawl}\")\n            try:\n                start_crawl = time.perf_counter()\n                crawled_results = await asyncio.to_thread(exa.get_contents, [url_to_crawl])\n                elapsed_crawl = time.perf_counter() - start_crawl\n                content = (\n                    crawled_results.results[0].text\n                    if crawled_results.results\n                    else \"Could not retrieve any text content from the URL.\"\n                )\n                await _status(\"Crawl complete.\", done=True)\n                self.debug.crawl(f\"Successfully crawled {len(content)} characters in {elapsed_crawl:.2f}s\")\n\n                # Simple CRAWL mode debug report\n                if self.debug.enabled:\n                    crawl_report = [\n                        \"\\n\\n\" + \"=\" * 35 + \" CRAWL MODE DEBUG REPORT \" + \"=\" * 35,\n                        f\"URL REQUESTED: {url_to_crawl}\",\n                        f\"CRAWL SUCCESS: \u2713 Yes\",\n                        f\"CONTENT LENGTH: {len(content)} characters\",\n                        f\"CONTENT PREVIEW:\\n{content[:300]}{'...' if len(content) > 300 else ''}\",\n                        \"\",\n                        \"=\" * 96 + \"\\n\",\n                    ]\n                    self.debug.report(\"\\n\".join(crawl_report))\n                    self.debug.content_metrics(len(content))\n                    self.debug.metrics_summary()\n\n                self.debug.synthesis(\"Crawl complete, returning content.\")\n                return {\n                    \"content\": content,\n                    \"show_source\": show_sources,\n                }\n            except Exception as e:\n                self.debug.error(f\"Crawl failed: {e}\")\n\n                # CRAWL mode failure debug report\n                if self.debug.enabled:\n                    crawl_report = [\n                        \"\\n\\n\" + \"=\" * 35 + \" CRAWL MODE DEBUG REPORT \" + \"=\" * 35,\n                        f\"URL REQUESTED: {url_to_crawl}\",\n                        f\"CRAWL SUCCESS: \u2717 No\",\n                        f\"ERROR: {str(e)}\",\n                        \"\",\n                        \"=\" * 96 + \"\\n\",\n                    ]\n                    self.debug.report(\"\\n\".join(crawl_report))\n                    self.debug.url_metrics(failed=1)\n                    self.debug.metrics_summary()\n\n                return {\n                    \"content\": f\"I failed while trying to crawl the URL: {e}\",\n                    \"show_source\": show_sources,\n                }\n\n        # Mode 2 - Standard\n        elif decision == \"STANDARD\":\n            # Check if custom standard search is enabled\n            if self.valves.use_custom_standard_search:\n                self.debug.flow(\"Custom standard search enabled, routing to agentic search\")\n                return await self._custom_standard_search(\n                    last_user_message, convo_snippet, _status, __request__, user_obj, show_sources\n                )\n\n            # Default standard search pipeline\n            self._last_error = None  # Clear any previous errors\n            self.debug.flow(\"Starting STANDARD search mode (default pipeline)\")\n            report = QuickDebugReport(\n                initial_query=last_user_message,\n                valve_urls_to_search=self.valves.quick_urls_to_search,\n                valve_queries_to_crawl=self.valves.quick_queries_to_crawl,\n                valve_max_context_chars=self.valves.quick_max_context_chars,\n            )\n            final_result = \"\"\n            context = \"\"\n            try:\n                await _status(\"Formulating search plan...\")\n\n                refiner_user_prompt = f\"## Conversation History:\\n{convo_snippet}\\n\\n## User's Latest Query:\\n'{last_user_message}'\"\n                refiner_payload = {\n                    \"model\": self.valves.quick_search_model,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": IMPROVED_SQR_PROMPT_TEMPLATE},\n                        {\"role\": \"user\", \"content\": refiner_user_prompt},\n                    ],\n                    \"stream\": False,\n                }\n\n                res = await generate_with_retry(\n                    request=__request__,\n                    form_data=refiner_payload,\n                    user=user_obj,\n                    debug=self.debug,\n                )\n                llm_response_text = res[\"choices\"][0][\"message\"][\"content\"].strip()\n                self.debug.data(\"SQR full response\", llm_response_text, truncate=200)\n\n                # Extract refined query using correction helper\n                refined_query = await self._extract_with_correction(\n                    request=__request__,\n                    user_obj=user_obj,\n                    model=self.valves.quick_search_model,\n                    original_text=llm_response_text,\n                    prefix=\"ANSWER:\",\n                    validate=lambda s: len(s) > 0 and len(s) <= 1000,\n                    correction_instructions=(\n                        \"Return exactly one line starting with 'ANSWER: ' followed by the optimized search query. \"\n                        \"No explanations or quotes.\"\n                    ),\n                )\n                self.debug.query(f\"Refined STANDARD query: {refined_query}\")\n                report.refined_query = refined_query\n\n                await _status(f'Searching for: \"{refined_query}\"')\n\n                # Use safe search method\n                search_results = await self._safe_exa_search(\n                    refined_query, self.valves.quick_urls_to_search, \"STANDARD search\"\n                )\n\n                if not search_results:\n                    if self._last_error:\n                        final_result = f\"Search failed with error: {self._last_error}\"\n                    else:\n                        final_result = \"My search found no results to read. Please try a different query.\"\n                    self.debug.search(\"No search results found\")\n                else:\n                    report.urls_found = [res.url for res in search_results]\n                    self.debug.search(f\"Found {len(report.urls_found)} search results\")\n\n                    crawl_candidates = search_results[\n                        : self.valves.quick_queries_to_crawl\n                    ]\n\n                    if not crawl_candidates:\n                        if self._last_error:\n                            final_result = f\"Search succeeded but failed to retrieve content: {self._last_error}\"\n                        else:\n                            final_result = \"My search found no results to read. Please try a different query.\"\n                        self.debug.search(\"No crawl candidates found\")\n                    else:\n                        domains = [\n                            urlparse(res.url).netloc.replace(\"www.\", \"\")\n                            for res in crawl_candidates\n                        ]\n                        await _status(f\"Reading from: {', '.join(domains)}\")\n                        self.debug.crawl(\n                            f\"Crawling {len(crawl_candidates)} URLs from domains: {domains}\"\n                        )\n\n                        ids_to_crawl = [res.id for res in crawl_candidates]\n                        report.urls_crawled = [res.url for res in crawl_candidates]\n\n                        # Use safe crawl method\n                        crawled_results = await self._safe_exa_crawl(\n                            ids_to_crawl, \"STANDARD search\"\n                        )\n\n                        # Populate report with success/failure data\n                        successful_urls = [res.url for res in crawled_results]\n                        report.urls_successful = successful_urls\n\n                        failed_urls = [\n                            url\n                            for url in report.urls_crawled\n                            if url not in successful_urls\n                        ]\n                        report.urls_failed = failed_urls\n\n                        if crawled_results:\n                            await _status(\n                                f\"Synthesizing answer from {len(crawled_results)} sources...\"\n                            )\n                            context = \"\\n\\n\".join(\n                                [\n                                    f\"## Source: {res.url}\\n\\n{res.text}\"\n                                    for res in crawled_results\n                                ]\n                            )\n                            context = context[: self.valves.quick_max_context_chars]\n\n                            # Populate report metrics\n                            report.context_length = len(context)\n                            report.was_truncated = (\n                                len(\n                                    \"\\n\\n\".join(\n                                        [\n                                            f\"## Source: {res.url}\\n\\n{res.text}\"\n                                            for res in crawled_results\n                                        ]\n                                    )\n                                )\n                                > self.valves.quick_max_context_chars\n                            )\n\n                            self.debug.synthesis(\n                                f\"Generated context with {len(context)} characters from {len(crawled_results)} sources\"\n                            )\n\n                            summarizer_user_prompt = f\"## Context:\\n{context}\\n\\n## User's Question:\\n{last_user_message}\"\n                            report.final_prompt = f\"SYSTEM: {QUICK_SUMMARIZER_PROMPT}\\nUSER: {summarizer_user_prompt}\"\n                            summarizer_payload = {\n                                \"model\": self.valves.quick_search_model,\n                                \"messages\": [\n                                    {\n                                        \"role\": \"system\",\n                                        \"content\": QUICK_SUMMARIZER_PROMPT,\n                                    },\n                                    {\"role\": \"user\", \"content\": summarizer_user_prompt},\n                                ],\n                                \"stream\": False,\n                            }\n                            final_res = await generate_with_retry(\n                                request=__request__,\n                                form_data=summarizer_payload,\n                                user=user_obj,\n                                debug=self.debug,\n                            )\n                            final_result = final_res[\"choices\"][0][\"message\"][\"content\"]\n                            await _status(\"Standard search complete.\", done=True)\n                            self.debug.synthesis(\"STANDARD search synthesis complete\")\n                        else:\n                            if self._last_error:\n                                final_result = f\"I found search results but failed to read content: {self._last_error}\"\n                            else:\n                                final_result = \"I found search results but was unable to read any content from them. Please try a different query.\"\n\n            except Exception as e:\n                self.debug.error(f\"STANDARD search path failed with an exception: {e}\")\n                if context:\n                    final_result = f\"I found some information but encountered an error while processing it. Here is the raw data I gathered:\\n\\n{context}\"\n                else:\n                    final_result = f\"I failed during the standard search: {e}\"\n            finally:\n                report.final_output = final_result\n                if self.debug.enabled:\n                    self.debug.report(report.format_report())\n                    self.debug.metrics_summary()\n                return {\"content\": final_result, \"show_source\": show_sources}\n\n        # Mode 3 - Complete (New Iterative Research System)\n        elif decision == \"COMPLETE\":\n            return await self._iterative_complete_search(\n                last_user_message, convo_snippet, _status, __request__, user_obj, show_sources\n            )\n\n        self.debug.flow(\"SearchRouterTool processing completed\")\n        if self.debug.enabled:\n            self.debug.metrics_summary()\n        return {\n            \"content\": f\"Router chose '{decision}', but no corresponding action was taken.\",\n            \"show_source\": show_sources,\n        }\n\n    async def _custom_standard_search(\n        self, user_query: str, convo_snippet: str, status_func, request, user_obj, show_sources: bool\n    ) -> dict:\n        \"\"\"\n        Custom agentic standard search with unified agent loop for\n        extraction, evaluation, and query generation in a single call.\n        \"\"\"\n        self._last_error = None\n        self.debug.flow(\"Starting CUSTOM standard search mode (unified)\")\n\n        try:\n            from datetime import datetime\n            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n\n            # Configuration\n            max_iterations = self.valves.custom_search_max_iterations\n            queries_per_iter = self.valves.custom_search_queries_per_iteration\n\n            all_findings = []\n            all_sources = []\n            iteration = 0\n            final_agent_response = None  # Will hold the response when agent decides STOP\n\n            # Start with initial query based on user request\n            queries = [user_query]\n\n            while iteration < max_iterations:\n                iteration += 1\n                self.debug.iteration(f\"Custom search iteration {iteration}/{max_iterations}\")\n\n                # \u2500\u2500\u2500 Execute searches and crawl \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                iteration_content = []\n\n                for query in queries:\n                    self.debug.search(f\"Executing search: {query[:50]}...\")\n                    await status_func(f\"Searching: {query[:40]}...\")\n\n                    search_results = await self._safe_exa_search(\n                        query,\n                        self.valves.custom_search_urls_per_query,\n                        f\"custom_iter_{iteration}\"\n                    )\n\n                    if not search_results:\n                        self.debug.warning(f\"No results for query: {query[:50]}\")\n                        continue\n\n                    ids_to_crawl = [res.id for res in search_results[:self.valves.custom_search_urls_to_crawl]]\n                    await status_func(f\"Reading {len(ids_to_crawl)} sources...\")\n\n                    crawled_results = await self._safe_exa_crawl(\n                        ids_to_crawl,\n                        f\"custom_iter_{iteration}\"\n                    )\n\n                    if crawled_results:\n                        for res in crawled_results:\n                            if getattr(res, 'text', None):\n                                text = ' '.join(res.text.split()[:1500])\n                                source_url = getattr(res, 'url', 'Unknown')\n                                iteration_content.append(f\"[Source: {source_url}]\\n{text}\")\n                                if source_url not in all_sources:\n                                    all_sources.append(source_url)\n\n                # \u2500\u2500\u2500 Unified agent call: extract, evaluate, decide \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n                await status_func(\"Analyzing findings...\")\n\n                # Build previous findings section\n                if all_findings:\n                    previous_findings = f\"## Previous Findings:\\n{chr(10).join(all_findings)}\"\n                else:\n                    previous_findings = \"## Previous Findings:\\nNone yet - this is the first search.\"\n\n                # Build new content section\n                if iteration_content:\n                    new_content = \"\\n\\n---\\n\\n\".join(iteration_content[:6])\n                    new_content_section = f\"## New Crawled Content (from this iteration):\\n{new_content[:12000]}\"\n                else:\n                    new_content_section = \"## New Crawled Content:\\nNo content retrieved this iteration.\"\n\n                agent_prompt = CUSTOM_SEARCH_AGENT_PROMPT.format(\n                    current_date=current_date,\n                    user_request=user_query,\n                    current_iteration=iteration,\n                    max_iterations=max_iterations,\n                    query_count=queries_per_iter,\n                    source_count=len(all_sources),\n                    previous_findings=previous_findings,\n                    new_content_section=new_content_section\n                )\n\n                agent_payload = {\n                    \"model\": self.valves.custom_search_agent_model,\n                    \"messages\": [\n                        {\"role\": \"user\", \"content\": agent_prompt},\n                    ],\n                    \"stream\": False,\n                }\n\n                agent_res = await generate_with_parsing_retry(\n                    request=request,\n                    form_data=agent_payload,\n                    user=user_obj,\n                    debug=self.debug,\n                    expected_keys=[\"choices\", \"content\", \"message\"]\n                )\n\n                # Extract response\n                if \"choices\" in agent_res and agent_res[\"choices\"]:\n                    agent_response = agent_res[\"choices\"][0][\"message\"][\"content\"]\n                elif \"content\" in agent_res:\n                    agent_response = agent_res[\"content\"]\n                elif isinstance(agent_res, str):\n                    agent_response = agent_res\n                else:\n                    agent_response = str(agent_res)\n\n                self.debug.data(\"Agent response\", agent_response, truncate=300)\n\n                # Extract status for UI\n                for line in agent_response.split(\"\\n\"):\n                    if \"STATUS_SUMMARY:\" in line.upper():\n                        await status_func(line.split(\":\", 1)[1].strip()[:60])\n                        break\n\n                # Extract multi-line findings using section parsing\n                extracted_info = self._extract_section(agent_response, \"EXTRACTED_INFO:\")\n\n                if extracted_info:\n                    self.debug.data(f\"Extracted info (iter {iteration})\", extracted_info[:300], truncate=300)\n                else:\n                    self.debug.warning(f\"No EXTRACTED_INFO found in iteration {iteration}\")\n                    self.debug.data(\"Full agent response for debug\", agent_response[:500], truncate=500)\n\n                if extracted_info and extracted_info.lower() not in [\"no new content\", \"none\", \"\"]:\n                    all_findings.append(f\"[Iteration {iteration}]\\n{extracted_info}\")\n                    self.debug.flow(f\"Added findings from iteration {iteration}, total findings: {len(all_findings)}\")\n                else:\n                    self.debug.warning(f\"Skipped adding findings - extracted_info was: '{extracted_info[:50] if extracted_info else 'empty'}'\")\n\n                # Extract decision\n                decision = \"CONTINUE\"\n                for line in agent_response.split(\"\\n\"):\n                    if \"DECISION:\" in line.upper():\n                        if \"STOP\" in line.upper():\n                            decision = \"STOP\"\n                        break\n\n                self.debug.agent(f\"Iteration {iteration} decision: {decision}\")\n\n                if decision == \"STOP\":\n                    self.debug.flow(\"Agent determined information is sufficient\")\n                    # Store the final response containing the summary\n                    final_agent_response = agent_response\n                    break\n\n                # Extract next queries if continuing\n                if iteration < max_iterations:\n                    queries = []\n                    for line in agent_response.split(\"\\n\"):\n                        if \"NEXT_QUERIES:\" in line.upper():\n                            try:\n                                query_part = line.split(\":\", 1)[1].strip()\n                                if query_part.startswith(\"[\"):\n                                    queries = json.loads(query_part)\n                            except (json.JSONDecodeError, IndexError):\n                                pass\n                            break\n\n                    # Fallback: extract quoted strings\n                    if not queries:\n                        quoted = re.findall(r'\"([^\"]{5,})\"', agent_response)\n                        queries = [q for q in quoted if q.lower() not in [\"stop\", \"continue\"]][:queries_per_iter]\n\n                    if not queries:\n                        self.debug.warning(\"No next queries found, stopping\")\n                        break\n\n                    queries = queries[:queries_per_iter]\n                    await status_func(\"Searching for more info...\")\n\n            # \u2500\u2500\u2500 Format final output \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n            await status_func(\"Compiling research results...\")\n            self.debug.flow(f\"Final output: {len(all_findings)} findings, {len(all_sources)} sources\")\n\n            if not all_findings:\n                self.debug.warning(\"No findings collected - returning error message\")\n                return {\n                    \"content\": \"I was unable to find relevant information for your request. Please try rephrasing your question.\",\n                    \"show_source\": show_sources,\n                }\n\n            # Extract summary from agent's final response (no separate LLM call needed)\n            if final_agent_response:\n                # Agent decided STOP - extract RESEARCH_SUMMARY and KEY_POINTS from its response\n                self.debug.flow(\"Extracting summary from agent's STOP response\")\n                research_summary = self._extract_section(final_agent_response, \"RESEARCH_SUMMARY:\")\n                key_points = self._extract_section(final_agent_response, \"KEY_POINTS:\")\n\n                if research_summary:\n                    final_content = f\"RESEARCH_SUMMARY:\\n{research_summary}\"\n                    if key_points:\n                        final_content += f\"\\n\\nKEY_POINTS:\\n{key_points}\"\n                    final_content += f\"\\n\\nSOURCES_CONSULTED: {len(all_sources)}\"\n                    self.debug.data(\"Extracted research summary\", research_summary[:200], truncate=200)\n                else:\n                    # Fallback: use accumulated findings if summary extraction failed\n                    self.debug.warning(\"Could not extract RESEARCH_SUMMARY from agent response, using raw findings\")\n                    final_content = f\"RESEARCH_FINDINGS:\\n\" + \"\\n\\n\".join(all_findings)\n                    final_content += f\"\\n\\nSOURCES_CONSULTED: {len(all_sources)}\"\n            else:\n                # Max iterations reached without STOP - use accumulated findings\n                self.debug.flow(\"Max iterations reached, using accumulated findings\")\n                final_content = f\"RESEARCH_FINDINGS:\\n\" + \"\\n\\n\".join(all_findings)\n                final_content += f\"\\n\\nSOURCES_CONSULTED: {len(all_sources)}\"\n\n            await status_func(\"Search complete.\", done=True)\n            self.debug.synthesis(f\"Custom standard search complete. {len(all_sources)} sources, {iteration} iterations.\")\n            self.debug.data(\"Final content preview\", final_content[:300] if final_content else \"EMPTY\", truncate=300)\n\n            if self.debug.enabled:\n                self.debug.metrics_summary()\n\n            return {\n                \"content\": final_content,\n                \"show_source\": show_sources,\n            }\n\n        except Exception as e:\n            self.debug.error(f\"Custom standard search failed: {e}\")\n            if self.debug.enabled:\n                self.debug.metrics_summary()\n            try:\n                await status_func(\"\", done=True)\n            except Exception:\n                pass\n            return {\n                \"content\": f\"I encountered an error during the search: {e}\",\n                \"show_source\": show_sources,\n            }\n\n    async def _iterative_complete_search(\n        self, user_query: str, convo_snippet: str, status_func, request, user_obj, show_sources: bool\n    ) -> dict:\n        \"\"\"New iterative complete search system based on user's vision\"\"\"\n        self._last_error = None  # Clear any previous errors\n        self.debug.flow(\"Starting NEW iterative complete search mode\")\n        \n        try:\n            # Get current datetime for all prompts\n            from datetime import datetime\n            current_date = datetime.now().strftime(\"%Y-%m-%d\")\n            current_year = datetime.now().year\n            \n            # Phase 1: Generate introductory query for context\n            await status_func(\"Gathering initial context...\")\n            self.debug.agent(\"Phase 1: Generating introductory query\")\n            \n            intro_payload = {\n                \"model\": self.valves.complete_agent_model,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": INTRODUCTORY_QUERY_PROMPT.format(current_date=current_date, current_year=current_year)},\n                    {\"role\": \"user\", \"content\": f\"User's request: {user_query}\"},\n                ],\n                \"stream\": False,\n            }\n            \n            intro_res = await generate_with_parsing_retry(\n                request=request, \n                form_data=intro_payload, \n                user=user_obj, \n                debug=self.debug,\n                expected_keys=[\"choices\", \"content\", \"message\"]\n            )\n            \n            # Debug the response structure\n            self.debug.data(\"Intro LLM raw response type\", type(intro_res))\n            self.debug.data(\"Intro LLM raw response keys\", list(intro_res.keys()) if isinstance(intro_res, dict) else \"Not a dict\")\n            self.debug.data(\"Intro LLM full response\", str(intro_res)[:800] + \"...\" if len(str(intro_res)) > 800 else str(intro_res))\n            \n            # Handle different response formats\n            if \"choices\" in intro_res and intro_res[\"choices\"]:\n                intro_response = intro_res[\"choices\"][0][\"message\"][\"content\"]\n            elif \"content\" in intro_res:\n                intro_response = intro_res[\"content\"]\n            elif \"message\" in intro_res:\n                intro_response = intro_res[\"message\"]\n            elif isinstance(intro_res, str):\n                intro_response = intro_res\n            else:\n                raise ValueError(f\"Unexpected intro LLM response format. Keys: {list(intro_res.keys()) if isinstance(intro_res, dict) else 'Not a dict'}\")\n            \n            # Extract intro query using correction helper\n            intro_query = await self._extract_with_correction(\n                request=request,\n                user_obj=user_obj,\n                model=self.valves.complete_agent_model,\n                original_text=intro_response,\n                prefix=\"QUERY:\",\n                validate=lambda s: len(s) > 0,\n                correction_instructions=(\n                    \"Return exactly one line starting with 'QUERY: ' followed by a concise introductory search query. \"\n                    \"No additional text.\"\n                ),\n            )\n            \n            self.debug.data(\"Introductory query extracted\", intro_query)\n            \n            # Search with the introductory query - with retry logic for failures\n            intro_content = \"\"\n            intro_search_attempts = 0\n            max_intro_attempts = 3\n            \n            while intro_search_attempts < max_intro_attempts and not intro_content:\n                intro_search_attempts += 1\n                self.debug.search(f\"Executing introductory search (attempt {intro_search_attempts}) \u2014 query: {intro_query}\")\n                \n                try:\n                    # Perform search + crawl using internal helpers (replaces missing _search_and_crawl)\n                    search_results = await self._safe_exa_search(\n                        intro_query, self.valves.complete_urls_to_search_per_query, \"introductory\"\n                    )\n                    intro_results = {\"content\": \"\", \"sources\": []}\n                    if search_results:\n                        ids_to_crawl = [res.id for res in search_results[: self.valves.complete_queries_to_crawl]]\n                        crawled_results = await self._safe_exa_crawl(ids_to_crawl, \"introductory\")\n                        if crawled_results:\n                            texts = []\n                            for res in crawled_results:\n                                if getattr(res, \"text\", None):\n                                    texts.append(' '.join(res.text.split()[:3000]))\n                                if getattr(res, \"url\", None):\n                                    intro_results[\"sources\"].append(res.url)\n                            intro_results[\"content\"] = \"\\n\\n\".join(texts)\n                    intro_content = intro_results.get(\"content\", \"\")\n                    \n                    if intro_content and intro_content.strip():\n                        if intro_results.get(\"sources\"):\n                            self.debug.data(\"Intro search sources found\", len(intro_results[\"sources\"]))\n                        break\n                    else:\n                        self.debug.error(f\"Introductory search attempt {intro_search_attempts} returned empty content\")\n                        if intro_search_attempts < max_intro_attempts:\n                            # Modify query slightly for retry\n                            intro_query = f\"{intro_query} overview basics\"\n                            await status_func(f\"Retrying initial context search (attempt {intro_search_attempts + 1})...\")\n                        \n                except Exception as e:\n                    self.debug.error(f\"Introductory search attempt {intro_search_attempts} failed: {str(e)}\")\n                    if intro_search_attempts < max_intro_attempts:\n                        await status_func(f\"Retrying initial context search (attempt {intro_search_attempts + 1})...\")\n            \n            if not intro_content:\n                intro_content = \"Unable to gather initial context after multiple attempts. Proceeding with user query directly.\"\n                self.debug.warning(\"All introductory search attempts failed, using fallback\")\n            \n            # Phase 2: Set objectives and research direction\n            await status_func(\"Setting research objectives...\")\n            self.debug.agent(\"Phase 2: Setting research objectives\")\n            \n            objectives_payload = {\n                \"model\": self.valves.complete_agent_model,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": OBJECTIVE_SETTING_PROMPT.format(current_date=current_date)},\n                    {\"role\": \"user\", \"content\": f\"User's request: {user_query}\\n\\nConversation context: {convo_snippet}\\n\\nIntroductory information:\\n{intro_content}\"},\n                ],\n                \"stream\": False,\n            }\n            \n            objectives_res = await generate_with_parsing_retry(\n                request=request, \n                form_data=objectives_payload, \n                user=user_obj, \n                debug=self.debug,\n                expected_keys=[\"choices\", \"content\", \"message\"]\n            )\n            \n            # Handle different response formats\n            if \"choices\" in objectives_res and objectives_res[\"choices\"]:\n                objectives_response = objectives_res[\"choices\"][0][\"message\"][\"content\"]\n            elif \"content\" in objectives_res:\n                objectives_response = objectives_res[\"content\"]\n            elif \"message\" in objectives_res:\n                objectives_response = objectives_res[\"message\"]\n            elif isinstance(objectives_res, str):\n                objectives_response = objectives_res\n            else:\n                raise ValueError(f\"Unexpected objectives LLM response format. Keys: {list(objectives_res.keys()) if isinstance(objectives_res, dict) else 'Not a dict'}\")\n            self.debug.data(\"Research objectives\", objectives_response, truncate=300)\n            \n            # Phase 3: Iterative search with reasoning\n            research_chain = [f\"INITIAL CONTEXT: {intro_content[:10000]}...\", f\"OBJECTIVES: {objectives_response}\"]\n            previous_findings = \"Initial context gathered from introductory search.\"\n            \n            for iteration in range(1, self.valves.complete_max_search_iterations + 1):\n                self.debug.iteration(f\"Starting research iteration {iteration}/{self.valves.complete_max_search_iterations}\")\n                await status_func(f\"Research iteration {iteration}/{self.valves.complete_max_search_iterations}...\")\n                \n                # Generate reasoning and queries for this iteration\n                reasoning_prompt = ITERATION_REASONING_PROMPT.format(\n                    current_date=current_date,\n                    current_year=current_year,\n                    objectives=objectives_response[:6000],\n                    previous_findings=previous_findings[:16000],\n                    current_iteration=iteration,\n                    max_iterations=self.valves.complete_max_search_iterations,\n                    query_count=self.valves.complete_queries_to_generate\n                )\n                \n                reasoning_payload = {\n                    \"model\": self.valves.complete_agent_model,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": \"You are a research iteration planner.\"},\n                        {\"role\": \"user\", \"content\": reasoning_prompt},\n                    ],\n                    \"stream\": False,\n                }\n                \n                reasoning_res = await generate_with_parsing_retry(\n                    request=request, \n                    form_data=reasoning_payload, \n                    user=user_obj, \n                    debug=self.debug,\n                    expected_keys=[\"choices\", \"content\", \"message\"]\n                )\n                \n                # Handle different response formats\n                if \"choices\" in reasoning_res and reasoning_res[\"choices\"]:\n                    reasoning_response = reasoning_res[\"choices\"][0][\"message\"][\"content\"]\n                elif \"content\" in reasoning_res:\n                    reasoning_response = reasoning_res[\"content\"]\n                elif \"message\" in reasoning_res:\n                    reasoning_response = reasoning_res[\"message\"]\n                elif isinstance(reasoning_res, str):\n                    reasoning_response = reasoning_res\n                else:\n                    raise ValueError(f\"Unexpected reasoning LLM response format. Keys: {list(reasoning_res.keys()) if isinstance(reasoning_res, dict) else 'Not a dict'}\")\n                self.debug.agent(f\"Iteration {iteration} reasoning: {reasoning_response[:200]}...\")\n                \n                # Extract queries from reasoning response with more robust parsing\n                queries = []\n                \n                # Method 1: Look for QUERIES: line and extract from it\n                for line in reasoning_response.split(\"\\n\"):\n                    if \"QUERIES:\" in line.upper():\n                        # Try to extract from same line if it contains array-like structure\n                        if \"[\" in line and \"]\" in line:\n                            try:\n                                import json\n                                query_part = line.split(\"QUERIES:\")[-1].strip()\n                                queries = json.loads(query_part)\n                                break\n                            except json.JSONDecodeError:\n                                continue\n                        \n                        # Look at following lines for array structure\n                        lines = reasoning_response.split(\"\\n\")\n                        start_idx = lines.index(line)\n                        for i in range(start_idx + 1, min(start_idx + 10, len(lines))):\n                            current_line = lines[i].strip()\n                            if current_line.startswith(\"[\") and \"]\" in current_line:\n                                try:\n                                    import json\n                                    queries = json.loads(current_line)\n                                    break\n                                except json.JSONDecodeError:\n                                    continue\n                        break\n                \n                # Method 2: Extract individual quoted strings\n                if not queries:\n                    import re\n                    quoted_queries = re.findall(r'\"([^\"]*)\"', reasoning_response)\n                    if quoted_queries:\n                        queries = [q for q in quoted_queries if len(q) > 5]  # Filter short quotes\n                \n                # Method 3: Look for list-style queries\n                if not queries:\n                    lines = reasoning_response.split(\"\\n\")\n                    for line in lines:\n                        if (line.strip().startswith(\"-\") or line.strip().startswith(\"*\") or \n                            line.strip().startswith(\"1.\") or line.strip().startswith(\"2.\")):\n                            query = line.strip().lstrip(\"-*123456789. \").strip()\n                            if len(query) > 10:\n                                queries.append(query)\n                \n                # Final fallback\n                if not queries:\n                    queries = [f\"{user_query} detailed research iteration {iteration}\"]\n                    self.debug.error(f\"Could not extract queries from reasoning, using fallback\")\n                \n                queries = queries[:self.valves.complete_queries_to_generate]  # Limit to valve setting\n                self.debug.search(f\"Iteration {iteration} queries: {queries}\")\n                \n                # Execute searches for this iteration\n                iteration_findings = []\n                for query in queries:\n                    await status_func(f\"Searching: {query[:40]}...\")\n                    \n                    search_results = await self._safe_exa_search(\n                        query, self.valves.complete_urls_to_search_per_query, f\"iteration {iteration} query\"\n                    )\n                    \n                    if search_results:\n                        ids_to_crawl = [res.id for res in search_results[:self.valves.complete_queries_to_crawl]]\n                        crawled_results = await self._safe_exa_crawl(ids_to_crawl, f\"iteration {iteration} crawl\")\n                        \n                        if crawled_results:\n                            for res in crawled_results:\n                                if res.text and res.text.strip():\n                                    title = res.title or \"Unknown Source\"\n                                    text_summary = ' '.join(res.text.split()[:3000])\n                                    if text_summary:\n                                        finding_summary = f\"From {title}: {text_summary}\"\n                                        iteration_findings.append(finding_summary)\n                \n                # Conclude iteration\n                await status_func(f\"Analyzing iteration {iteration} findings...\")\n                \n                iteration_content = \"\\n\\n\".join(iteration_findings) if iteration_findings else \"No significant findings in this iteration.\"\n                \n                conclusion_prompt = f\"\"\"\n                Research findings from iteration {iteration}:\n                {iteration_content}\n                \n                User's original question: {user_query}\n                Research objectives: {objectives_response}\n                Previous findings summary: {previous_findings}\n                \n                Analyze these findings and determine next steps.\n                \"\"\"\n                \n                conclusion_payload = {\n                    \"model\": self.valves.complete_agent_model,\n                    \"messages\": [\n                        {\"role\": \"system\", \"content\": ITERATION_CONCLUSION_PROMPT.format(\n                            current_date=current_date,\n                            current_iteration=iteration,\n                            max_iterations=self.valves.complete_max_search_iterations\n                        )},\n                        {\"role\": \"user\", \"content\": conclusion_prompt},\n                    ],\n                    \"stream\": False,\n                }\n                \n                conclusion_res = await generate_with_parsing_retry(\n                    request=request, \n                    form_data=conclusion_payload, \n                    user=user_obj, \n                    debug=self.debug,\n                    expected_keys=[\"choices\", \"content\", \"message\"]\n                )\n                \n                # Handle different response formats\n                if \"choices\" in conclusion_res and conclusion_res[\"choices\"]:\n                    conclusion_response = conclusion_res[\"choices\"][0][\"message\"][\"content\"]\n                elif \"content\" in conclusion_res:\n                    conclusion_response = conclusion_res[\"content\"]\n                elif \"message\" in conclusion_res:\n                    conclusion_response = conclusion_res[\"message\"]\n                elif isinstance(conclusion_res, str):\n                    conclusion_response = conclusion_res\n                else:\n                    raise ValueError(f\"Unexpected conclusion LLM response format. Keys: {list(conclusion_res.keys()) if isinstance(conclusion_res, dict) else 'Not a dict'}\")\n                \n                # Extract decision\n                decision = \"CONTINUE\"\n                if \"DECISION:\" in conclusion_response.upper():\n                    decision_line = [line for line in conclusion_response.split(\"\\n\") if \"DECISION:\" in line.upper()]\n                    if decision_line:\n                        decision = decision_line[0].split(\":\")[-1].strip().upper()\n                \n                # Update research chain (pass down summaries, not raw content)\n                findings_summary = \"No summary available\"\n                for line in conclusion_response.split(\"\\n\"):\n                    if \"FINDINGS_SUMMARY:\" in line.upper():\n                        findings_summary = line.split(\":\", 1)[1].strip()\n                        break\n                \n                research_chain.append(f\"ITERATION {iteration}: {findings_summary}\")\n                previous_findings = findings_summary\n                \n                self.debug.agent(f\"Iteration {iteration} decision: {decision}\")\n                \n                if decision == \"FINISH\" or iteration == self.valves.complete_max_search_iterations:\n                    self.debug.agent(\"Research concluded - synthesizing final answer\")\n                    break\n            \n            # Phase 4: Final synthesis\n            await status_func(\"Synthesizing comprehensive answer...\")\n            self.debug.synthesis(\"Generating final synthesis from research chain\")\n            \n            research_summary = \"\\n\\n\".join(research_chain)\n            \n            final_payload = {\n                \"model\": self.valves.complete_summarizer_model,\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": FINAL_SYNTHESIS_PROMPT.format(current_date=current_date)},\n                    {\"role\": \"user\", \"content\": f\"User's original question: {user_query}\\n\\nResearch progression and findings:\\n{research_summary}\"},\n                ],\n                \"stream\": False,\n            }\n            \n            final_res = await generate_with_parsing_retry(\n                request=request, \n                form_data=final_payload, \n                user=user_obj, \n                debug=self.debug,\n                expected_keys=[\"choices\", \"content\", \"message\"]\n            )\n            \n            # Handle different response formats\n            if \"choices\" in final_res and final_res[\"choices\"]:\n                final_answer = final_res[\"choices\"][0][\"message\"][\"content\"]\n            elif \"content\" in final_res:\n                final_answer = final_res[\"content\"]\n            elif \"message\" in final_res:\n                final_answer = final_res[\"message\"]\n            elif isinstance(final_res, str):\n                final_answer = final_res\n            else:\n                raise ValueError(f\"Unexpected final synthesis LLM response format. Keys: {list(final_res.keys()) if isinstance(final_res, dict) else 'Not a dict'}\")\n            \n            await status_func(\"Research complete.\", done=True)\n            self.debug.synthesis(\"Iterative complete search finished successfully\")\n            \n            if self.debug.enabled:\n                self.debug.metrics_summary()\n            \n            return {\"content\": final_answer, \"show_source\": show_sources}\n            \n        except Exception as e:\n            self.debug.error(f\"Iterative complete search failed: {e}\")\n            if self.debug.enabled:\n                self.debug.metrics_summary()\n            # Safety: clear any lingering status on failure\n            try:\n                await status_func(\"\", done=True)\n            except Exception:\n                pass\n            return {\n                \"content\": f\"I encountered an error during the research process: {e}\"\n            }\n\n\n# Final tool definition for OpenWebUI\nclass Tools:\n    \"\"\"Main class that OpenWebUI will use\"\"\"\n\n    def __init__(self):\n        self.tools_instance = ToolsInternal()\n        # Expose valves directly at the top level for OpenWebUI\n        self.valves = self.tools_instance.valves\n\n    class Valves(BaseModel):\n        exa_api_key: str = Field(default=\"\", description=\"Your Exa API key.\")\n        router_model: str = Field(\n            default=\"gpt-4o-mini\",\n            description=\"LLM for the initial CRAWL/STANDARD/COMPLETE decision.\",\n        )\n        quick_search_model: str = Field(\n            default=\"gpt-4o-mini\",\n            description=\"Single 'helper' model for all tasks in the STANDARD path (refining, summarizing).\",\n        )\n        complete_agent_model: str = Field(\n            default=\"gpt-4-turbo\",\n            description=\"The 'smart' model for all agentic steps in the COMPLETE path (refining, deciding, query generation).\",\n        )\n        complete_summarizer_model: str = Field(\n            default=\"gpt-4-turbo\",\n            description=\"Dedicated high-quality model for the final summary in the COMPLETE path.\",\n        )\n        quick_urls_to_search: int = Field(\n            default=5, description=\"Number of URLs to fetch for STANDARD search.\"\n        )\n        quick_queries_to_crawl: int = Field(\n            default=3, description=\"Number of top URLs to crawl for STANDARD search.\"\n        )\n        quick_max_context_chars: int = Field(\n            default=8000,\n            description=\"Maximum total characters of context to feed to the STANDARD search summarizer.\",\n        )\n        complete_urls_to_search_per_query: int = Field(\n            default=5,\n            description=\"Number of URLs to fetch for each targeted query in COMPLETE search.\",\n        )\n        complete_queries_to_crawl: int = Field(\n            default=3,\n            description=\"Number of top URLs to crawl for each targeted query in COMPLETE search.\",\n        )\n        complete_queries_to_generate: int = Field(\n            default=3,\n            description=\"Number of new targeted queries to generate per iteration.\",\n        )\n        complete_max_search_iterations: int = Field(\n            default=2, description=\"Maximum number of research loops for the agent.\"\n        )\n        # Custom Standard Search Settings\n        use_custom_standard_search: bool = Field(\n            default=False,\n            description=\"Use the new agentic custom search for STANDARD mode instead of the default pipeline.\",\n        )\n        custom_search_agent_model: str = Field(\n            default=\"gpt-4o-mini\",\n            description=\"Agent model for extraction, evaluation, and query generation in custom standard search.\",\n        )\n        custom_search_urls_per_query: int = Field(\n            default=5,\n            description=\"Number of URLs to find per search query in custom standard search.\",\n        )\n        custom_search_urls_to_crawl: int = Field(\n            default=3,\n            description=\"Number of top URLs to crawl per search query in custom standard search.\",\n        )\n        custom_search_max_iterations: int = Field(\n            default=3,\n            description=\"Maximum number of search iterations before returning results.\",\n        )\n        custom_search_queries_per_iteration: int = Field(\n            default=2,\n            description=\"Number of search queries to generate per iteration.\",\n        )\n        debug_enabled: bool = Field(\n            default=False,\n            description=\"Enable detailed debug logging for troubleshooting search operations.\",\n        )\n\n    async def routed_search(\n        self,\n        query: str,\n        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,\n        __request__: Optional[Any] = None,\n        __user__: Optional[Dict] = None,\n        __messages__: Optional[List[Dict]] = None,\n    ) -> dict:\n        # Create debug instance with consistent formatting\n        debug = Debug(enabled=self.valves.debug_enabled)\n        debug.flow(\"routed_search function called\")\n        debug.data(\"Query\", query[:50] + \"...\" if len(query) > 50 else query)\n\n        # Sync valve settings to internal instance\n        self.tools_instance.valves = self.valves\n\n        return await self.tools_instance.routed_search(\n            query, __event_emitter__, __request__, __user__, __messages__\n        )\n"
}